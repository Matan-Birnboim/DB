# Project 1 - PySpark | Distributed Data

Project utilizing PySpark to load, clean, extract insights, design a distributed DB based on the insights found, and deploy the design constraints. 

# Project 1 - Distributed Data

- Chose to share this project as an example, to reflect my thought process and coding abilities with regards to:

	- PySpark
	- Cleaning semistructured data
	- Extracting insights from data
	- Designing a distributed DB based on the insights found
	- Deploying the design using PySpark

## Files uploaded

- Data: 3 zipped files. (due to file upload limits, split into 3 zip files)
	- project_1_data_1: 4 csv files (users.csv, queries.csv, movies.csv, tickets.csv)
	- project_1_data_2: 1 csv file (credits_1.csv)
	- project_1_data_3: 1 csv file (credits_2.csv)


- part0 - 2 files (.py and PDF), run py file after extracting zipped data, merges credits_1 and credits_2 to credit.csv for the use through the project. Read PDF. 
- part1 - Extract and Clean: 2 files (.py and PDF)
- part2 - Data Insights: 2 files (.ipynb and .html)
- part3 - Design Distributed DB (PDF)
- part4 - Deploy Design (.py)



## How to review

- Extract all data from zipped files
- Run merge_credits.py (part0 directory py file)
- I recommend starting from reading the project assignment overview guidlines, to understand the project and the data (part0 directory PDF)
- Second, look into the data cleaning process (part1 directory)
- Third, review the insights found from within the data (part2 directory)
- Last, if the design of a distributed DB interests you, part3 & part4 directories are the place for you. 

- Depending on the field of expertise you would like to review, you can really start from anywhere...

## No code needed review (if you dont want to run any code files or extract zipped data)

- see part1 PDF file
- see part2 HTML file
- see part3 PDF
